ğŸ§ª test_pipeline.py â€” Manual Diagnostic Tool (Copy + Paste Ready)
This script simulates a full bop memory flow using mock inputs and outputs.
It does not write to your real memory system or call APIs â€” safe to run anytime.

ğŸ“„ test_pipeline.py
python
Copy
Edit
"""
ğŸ§ª test_pipeline.py â€” Manual System Validator

Simulates the entire BOP pipeline:
- Step 2: GPT â LLaMA input writer
- Step 3: LLaMA simulated JSON output
- Step 4: Save Manager file write
- Step 5: Drive Upload (mock)
- Step 6: FAISS Ingest (mock)

Safe to run â€” no memory files or APIs affected.
"""

import os
import json
from datetime import datetime

# Simulated file locations
FIXTURE_INPUT_PATH = "fixtures/mock_input.txt"
FIXTURE_LLaMA_OUTPUT = "fixtures/llama_output.json"
DUMMY_FILE_PATH = "TestZone/Dummy_Memory.txt"

# -------------------------------
# Step 2: Mock GPT â LLaMA Input Writer
def simulate_step2():
    with open(FIXTURE_INPUT_PATH, "r") as f:
        chunk = f.read()

    os.makedirs("llama", exist_ok=True)
    with open("llama/input.txt", "w") as f:
        f.write(f"# GENERATED ON: {datetime.now().isoformat()}\n")
        f.write("CHAT: Test Chat\nMSG_ID: msg_test_01\nHASH: test123hash\n\n")
        f.write("INSTRUCTION:\nSummarize and tag the following conversation.\n\n")
        f.write("CONTENT:\n" + chunk)

    print("[âœ…] Step 2: GPT â LLaMA input written to llama/input.txt")

# -------------------------------
# Step 3: Simulate LLaMA Output (using fixture)
def simulate_step3():
    os.makedirs("llama", exist_ok=True)
    with open(FIXTURE_LLaMA_OUTPUT, "r") as src:
        data = json.load(src)

    with open("llama/output.json", "w") as dst:
        json.dump(data, dst, indent=2)

    print("[âœ…] Step 3: Mock LLaMA output written to llama/output.json")

# -------------------------------
# Step 4: Simulate Save Manager Write
def simulate_step4():
    with open("llama/output.json") as f:
        memory = json.load(f)

    os.makedirs(os.path.dirname(DUMMY_FILE_PATH), exist_ok=True)
    with open(DUMMY_FILE_PATH, "a", encoding="utf-8") as f:
        f.write("\n\n" + memory["content"])

    print(f"[âœ…] Step 4: Memory appended to {DUMMY_FILE_PATH}")

# -------------------------------
# Step 5: Mock Drive Upload
def simulate_step5():
    print(f"[ğŸ›°ï¸] Step 5: Mock uploading '{DUMMY_FILE_PATH}' to Google Drive...")
    print("[âœ…] Upload simulated (no network call)")

# -------------------------------
# Step 6: Mock FAISS Ingest
def simulate_step6():
    print(f"[ğŸ§ ] Step 6: Mock ingesting '{DUMMY_FILE_PATH}' into FAISS...")
    print("[âœ…] Vector index simulation complete (no actual indexing)")

# -------------------------------
def run_pipeline():
    print("\nğŸ§ª Running test_pipeline.py â€” Simulated Memory Flow\n")
    simulate_step2()
    simulate_step3()
    simulate_step4()
    simulate_step5()
    simulate_step6()
    print("\nâœ… Test pipeline complete â€” system wiring verified.\n")

if __name__ == "__main__":
    run_pipeline()
ğŸ“‚ Folder Structure to Match
Create this minimal layout for fixtures:

pgsql
Copy
Edit
project_root/
â”œâ”€â”€ test_pipeline.py
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ mock_input.txt
â”‚   â””â”€â”€ llama_output.json
ğŸ“ mock_input.txt (example)
sql
Copy
Edit
User: So we agreed the Memory VDB should have changelogs per file and dedup checks before append, right?
Assistant: Correct. That will reduce clutter and ensure accurate reflection.
ğŸ“ llama_output.json (example)
json
Copy
Edit
{
  "summary": "Confirmed that the Memory VDB system requires changelogs and deduplication.",
  "tags": ["memory", "vdb", "deduplication", "changelog"],
  "timestamp": "2025-04-23T14:00:00Z",
  "source": "bop",
  "chat_name": "Test Chat",
  "message_id": "msg_test_01",
  "input_hash": "test123hash",
  "content": "Confirmed that changelogs and dedup checks are part of the Memory VDB flow."
}
âœ… Ready to Run
Once the above files are in place:

bash
Copy
Edit
python test_pipeline.py
Output will print to console â€” no real files, APIs, or vector indexes touched.

Let me know if you want the upgraded version to support live runs later â€” or want to move into Step 8 (retry logic + DAG-style flow).







