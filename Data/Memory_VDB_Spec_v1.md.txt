ğŸ§  Memory VDB System Specification â€” v1.1 (Upgraded)

ğŸŒŸ Purpose

The Memory VDB is a persistent, modular, and self-evolving long-term memory system designed for AI agents. It allows ChatGPT to:

Summarize and store memory from past conversations or tasks

Maintain organized, topic-specific knowledge files

Query past insights contextually

Evolve memory through intelligent updates, tagging, and archiving

It integrates local processing via LLaMA 3 (Ollama), structured file storage via Google Drive, and local FAISS-based vector search via the VDB Template.

âš™ï¸ Version 1.1 Enhancements

âœ… Key Fixes and Additions

Replaced fragile JSON regex with structured json.loads() scan

Added deduplication logic via input_hash.json

Introduced paths.json for modular path configuration

Enabled traceability with chat_name, message_id, and input_hash

Optional: semantic diff stub for append verification

Optional: token limit estimator before LLaMA processing

Introduced test harness plan and changelog per file/day

ğŸ§± System Architecture Overview

ChatGPT = Orchestrator (command interface, optional)LLaMA 3 via Ollama = Local summarizer, classifier, diff enginePython Script Layer = Executes file operations and indexingGoogle Drive = Source-of-truth file store (versioned .txt memory)VDB Template = FAISS vector database engine

Each memory zone is isolated in its own folder (/Master Memory, /Projects, /Life, etc.) based on the VDB Template structure.

ğŸ§¹ Core Components

ChatGPT (Optional UI/Orchestrator)

Triggers commands like bop, vdb:query, vdb:archive

Selects memory-worthy content (based on rules or checkpoints)

Orchestrates flow, but not required for all execution

LLaMA 3 (via Ollama)

Summarizes raw text

Tags entities, topics, system references

Performs semantic diff (if provided memory context)

Returns structured JSON (summary, tags, timestamp, action)

Google Drive

Stores finalized .txt memory files

Drive folder acts as versioned, auditable memory archive

VDB Template

ingest.py, query.py, vector_tools.py

Splits documents, embeds via OpenAI, stores vectors in FAISS index

Supports local CLI or Flask-based querying

Python Script Layer

Executes save_manager.py to apply LLaMA decisions

Manages file actions (append, replace, archive, upload)

Triggers ingestion scripts and changelogs

â¶ End-to-End Flow â€” bop Command

Trigger: ChatGPT receives bop command or checkpoint pattern

Selection: GPT extracts relevant messages (within token limit)

Hashing: GPT creates SHA256 hash of the memory block

Deduplication Check: If hash exists in input_hash.json, skip

Chunking: GPT truncates to ~3000 tokens

Metadata: Adds chat_name, message_id, timestamp to metadata

Write Input: llama/input.txt is created from template_path

Execution: Shell wrapper runs ollama run llama3 < input.txt > raw_output.txt

Postprocessing: Extracted JSON saved to llama/output.json

Save Action: save_manager.py modifies the correct .txt file

Upload: Final file posted to Drive

Ingest: ingest.py updates FAISS index

ğŸ›¡ï¸ Failure Points + Safeguards

ğŸ§¹ Step 1: bop Trigger

âœ… Token limit safeguard

âœ… Deduplication via input_hash.json

âœ… GPT chunk hash generation

ğŸ§¹ Step 2: GPT Input Writer

âœ… Path validation using paths.json

âœ… Sanitizer for markdown/quotes

âœ… Fallback template injection

ğŸ§¹ Step 3: LLaMA Execution

âœ… Structured JSON enforced line-by-line

âœ… Validates required fields (summary, tags, etc.)

ğŸ§¹ Step 4: Save Manager

âœ… Append/replace logic based on LLaMA decision

âœ… Hash check before file mutation

âœ… Dual logging (per file + global)

ğŸ§¹ Step 5: Upload

âœ… Retry logic on Drive error

âœ… Log all attempts in upload_log.json

ğŸ§¹ Step 6: Ingest

âœ… CLI watcher or auto-triggered

âœ… Validate OpenAI key/token thresholds

ğŸ“ Memory Structure + Evolution Strategy

Memory Zones:

/Core/, /Projects/, /Meta/, /Life/, /Ideas/, /Tech/, /Archive/

Memory Classes:

core, supporting, archivable

Commands:

vdb:review [file], vdb:append [file], vdb:replace [file], vdb:archive [file], vdb:promote [file]

ğŸ” LLaMA + Script-Based Division of Labor

Role

ChatGPT

LLaMA

Python

Chunk Input

âœ…

âŒ

âŒ

Summarize

âŒ

âœ…

âŒ

Tag

âŒ

âœ…

âŒ

Save Logic

âœ… (orchestrate)

âœ… (decide)

âœ… (execute)

Semantic Diff

âœ…

âœ… (if prompted)

âœ… (file diff)

ğŸ§  Self-Evolving Logic

ChatGPT parses past convos, triggers bop

Dedup check prevents wasted LLaMA calls

Metadata enriches memory traceability

Changelogs per file and daily summaries for observability

ğŸ§ª Optional (but implemented) Enhancements

paths.json for clean routing

input_hash.json for deduplication

Token counter for chunk size limit

Semantic diff planning for future overwrite detection

Test folder placeholder for Steps 2â€“4 validation

Last Updated: 2025-04-23 â€” v1.1 hardened upgrade
Architect: ChatGPT + [User]

